{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4900004901406540016\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8794303172246249917\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from fetch_data import fetch_events, clear_events, load_csv_and_create_dataframe, load_credentials\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import PrettyPrinter\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D, Flatten, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "plt.style.use('dark_background')\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_multiple_days (credentials, *days, equalize=False, for_review=False):\n",
    "    \n",
    "    purchases, regular_events = clear_events(fetch_events(credentials['user'], credentials['pwd'], days[0]).json()[0], for_review=for_review)\n",
    "    \n",
    "    data = load_csv_and_create_dataframe(purchases, 1)\n",
    "    if equalize:\n",
    "        data = load_csv_and_create_dataframe(sorted(regular_events, key=lambda x:random.random())[:len(purchases)], 0, data)\n",
    "    else:\n",
    "        data = load_csv_and_create_dataframe(regular_events, 0, data)\n",
    "        \n",
    "    for day in days[1:]:\n",
    "        purchases, regular_events = clear_events(fetch_events(credentials['user'], credentials['pwd'], day).json()[0], for_review=for_review)\n",
    "        data = load_csv_and_create_dataframe(purchases, 1, data)\n",
    "        if equalize:\n",
    "            data = load_csv_and_create_dataframe(sorted(regular_events, key=lambda x:random.random())[:len(purchases)], 0, data)\n",
    "        else:\n",
    "            data = load_csv_and_create_dataframe(regular_events, 0, data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x_data(data): \n",
    "    data['pos'] = data['pos'].apply(lambda x: np.array([list(map(float, value.replace('(','[').replace(')',']').replace('[','').replace(']','').split(', '))) for value in x]))\n",
    "    data['len'] = data['pos'].apply(len)\n",
    "    data['x'] = data['pos'].apply(lambda x: [data[0] for data in x])\n",
    "    data['y'] = data['pos'].apply(lambda x: [data[1] for data in x])\n",
    "    data['sizex'] = data['pos'].apply(lambda x: [data[2] for data in x])\n",
    "    data['sizey'] = data['pos'].apply(lambda x: [data[3] for data in x])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain_data(data, max_phrase_len):\n",
    "    X_x_train = pad_sequences(data['x'], maxlen = max_phrase_len) \n",
    "    X_y_train = pad_sequences(data['y'], maxlen = max_phrase_len)\n",
    "    X_szx_train = pad_sequences(data['sizex'], maxlen = max_phrase_len)\n",
    "    X_szy_train = pad_sequences(data['sizey'], maxlen = max_phrase_len)\n",
    "\n",
    "    x_train = np.array(list(zip(X_x_train, X_y_train, X_szx_train, X_szy_train)))\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/iago/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm = load_model('purchase_classification_20200205.h5')\n",
    "model_json = model_lstm.to_json()\n",
    "with open(\"purchase_classification_20200205.json\", \"w\") as json_file: json_file.write(model_json)\n",
    "max_phrase_len = 3000\n",
    "day = '2020-02-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login succed\n",
      "Login succed\n"
     ]
    }
   ],
   "source": [
    "load_credentials()\n",
    "credentials = {}\n",
    "with open('credentials_visio.json') as creds:\n",
    "    credentials = json.load(creds)\n",
    "\n",
    "events = fetch_events(credentials['user'], credentials['pwd'], day).json()[0]\n",
    "events_df = fetch_multiple_days(credentials, day, equalize=False, for_review=True)\n",
    "bench_data = constrain_data(generate_x_data(events_df), max_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_lstm.predict(bench_data)\n",
    "eventos_rejeitados = []\n",
    "\n",
    "for pred, event in zip(y_pred, events_df['id']):\n",
    "    if pred[0] > 0.95:\n",
    "        eventos_rejeitados.append(event)\n",
    "        \n",
    "eventos = {\n",
    "    \"_id\": day.replace('-',''),\n",
    "    \"eventos_aceitos\": [\n",
    "    ],\n",
    "    \"eventos_rejeitados\": [\n",
    "    ],\n",
    "    \"eventos_rejeitados_filtro\": [\n",
    "    ],\n",
    "    \"eventos_nao_revisados\": [\n",
    "    ],\n",
    "    \"eventos_interesse\": [\n",
    "    ],\n",
    "    \"eventos_swap\": [\n",
    "    ]\n",
    "}\n",
    "\n",
    "for event in events['eventos_nao_revisados']:\n",
    "    if event['_id'] in eventos_rejeitados:\n",
    "        eventos['eventos_rejeitados'].append(event)\n",
    "    else:\n",
    "        eventos['eventos_nao_revisados'].append(event)\n",
    "        \n",
    "eventos['eventos_rejeitados_filtro'] = events['eventos_rejeitados_filtro'] \n",
    "eventos['eventos_interesse'] = events['eventos_interesse'] \n",
    "eventos['eventos_rejeitados_filtro'] = events['eventos_rejeitados_filtro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(eventos, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
